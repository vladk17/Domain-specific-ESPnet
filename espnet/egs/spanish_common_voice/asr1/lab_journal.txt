run.sh:
  1) set verbose=1 #line 18
  2) resume=        # Resume the training from snapshot #line 19
  3) TBD: verify we do not do spaceaugmentation at the first phase (preprocess_config=conf/specaug.yaml # line 25)
  4) study train.yaml params:  
     4.1) batch-bins should correspond to the number of gpus (default in librispeech is 15000000, current setting is 1000000)
     4.2) accum-grad: 4-default, current setting is 1
     4.3) epochs: 120-defualt, current 20
     4.4) model-module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E - TBD: study the code and model structure from the code
     4.5) meaning of many other params is not clear - TBD: clarify the meaning

  5) study lm.yaml params - TBD
  6) study decode.yaml params:
     6.1) beamsize 60-default, currently value is 5    
     6.2) ctc-weight vs. lm-weight, should they sumup to 1 or what's important is the ratio between them, or something else? TBD - check
  7) feature generation: # Generate the fbank features; by default 80-dimensional fbanks with pitch on each frame
     any ideas about speciality of spanish audio and corresponding tuning of the parameters of the feature generation for spanish?
  8) accelerate decoding by commenting out the line below #### use CPU for decoding
             8.1) #ngpu=0
             8.2) Set 1 or more values for --batchsize option in asr_recog.py to enable GPU decoding
             8.3) And execute the script (e.g., run.sh --stage 5 --ngpu 1)
             You’ll achieve significant speed improvement by using the GPU decoding
  9) TBD: clarify the meaning of "model to be used for decoding: 'model.acc.best' or 'model.loss.best'"
 10) TBD: prepare a schematic diagram of the pipeline and present it to the member of the project
 11) TBD: clarify what is CMVN (Cepstral mean and variance normalization), how is it calculated and how is it used? 
          remember: Mel-Frequency Cepstral Coefficients (MFCCs)
 12) TBD: exercise transfer learning on publically available data; do not wait for the gong's labeled data
 13) TBD: listen to the data audio files
 14) TBD: prepare a schematic diagram of the data transformations along the pipeline; debug the pipeline by verifications of the format and content correctness 
     of the data at each stage of the pipeline
 15) overfitting regularization loop
 16) stay in control at each stage of teh pipeline: 
     16.1) clearly understand the optimization cretarium at the stage; 
     16.2) measure and know the achieved optimization values
     16.3) do one stage a time, analyze the results; zero step is listen to the samples 
 17) in common voice, waveforms have significant portion os silence at the start and the end of the recording
 18) nbpe=5000 (this paramter defines the size of the vacabular for the stage 2 "Dictionary and Json Data Preparation") what is the recommended value of this param?
 19) bpemode=unigram define the model type in the stage 2 "Dictionary and Json Data Preparation"; what does it mean, what are the options, and what is the recommended model type?
     it is explained and implemented here https://arxiv.org/abs/1804.10959 and here https://github.com/google/sentencepiece and 
 20) stage 2: what is the meaning of the param --input_sentence_size=100000000
 21) stage 3: LM: lm.yaml model-module: transformer; epoch set to 20 while the default is 50; what are other options for model-module and preferable configuration settings?
 22) stage 3: get the exact meaning of the paramters in the log output: main/loss, main/count, main/nll
 23) stage 3: starting from 6th epoch the model starts to overfit in the case of small (5000 taken for development) dataset; echieved train perplexity is 57.3077, echieved 
     validation perplexity is 193.023 
 24) paper: A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATION (https://arxiv.org/abs/1909.06317) provides training tips; it also reveals superiority of transformer
     in comparison to RNN; they prepared espnet recipes, which we actually use (espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py by Shigeki Karita)
     in these recipes the model is sequence2sequence model, which predicts a target sequence of subwords ("SentencePiece") from an input sequence of (83-dim) log-mel filterbank frames with pitch features.
     Questions: how sensitive the model to the selected subwords-dict? What are the pros and cons of setting the alphabet characters as a subwords-dict (the minimal (?) "full",so that any word can
     be created, subwords-dict)?
     
     see "A pitch extraction algorithm tuned for automatic speech recognition" by Povey et all

     see "SPEECH-TRANSFORMER: A NO-RECURRENCE SEQUENCE-TO-SEQUENCE MODEL FOR SPEECH RECOGNITION", which explains transformations of the source sequence into a subsampled sequence by using 
     two layer cnn 
     
 25) not sure that the sub-words are better than characters for the asr: the length of the subwords varies significantly (they can be pretty long), which seem to be problematic 
     for the acustic model as it outputs per time-frame. We may try "characters" and mixed "character" + "subwords" and compare performance
 26) train.yaml:  
     26.1) mtlalpha: 0.3 (Multitask learning mode alpha) defines the weights between loss_att and loss_ctc in the joint loss function L_asr = alpha * loss_ctc + (1 - alpha) * loss_att, 
           where loss_xyz = -log(P_xyz); TBD: clarify how P_xyz is defined/calculated 
           see "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING"
           Note: the values of loss_att ~ 14, while ctc_loss ~ 28 after 40 epochs, so the value of mtlalpha seem to be selected correctly or can be even smaller
     26.2) num_spkrs: 1 - in our current setting (this assumes that there is only a single speaker in each utterance)
     26.3) model_module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E
     26.4) elayers: 12  defines the number of blocks in the transformer's encoder
     26.5) dlayers: 6 defines tne number of blocks in the transformer's decoder
           it looks like the CTC log below outputs the input and output sequence lengths for each of the (6) dlayers 
           "
           2020-06-26 06:48:15,536 (ctc:83) INFO: CTC input lengths:  tensor([154, 154, 154, 154, 154, 154], dtype=torch.int32)
           2020-06-26 06:48:15,536 (ctc:84) INFO: CTC output lengths: tensor([17, 17, 16,  9, 15, 21], dtype=torch.int32)
           "
     26.6) adim: 512 defines attention_dim - attantion dimention
     26.7) aheads: 8 - number of attention heads
     26.8) TBD: check what is the value of positionwise_layer_type and corresponding positionwise_layer, which is given as a feed_forward paramter to the Transformer's EncoderLayer
           from the code it looks that in the current implementation the default value is taken: positionwise_layer_type="linear",
 27) define and implement a data-cleaning mechanism, so that a new experiment could be started from the "clean page" 
 28) define and implement data-saving mechanism, so that results of an experiment could be saved 
 29) getting lot's of following wordings: "sox WARN rate: rate clipped 12 samples; decrease volume?", changing the command to os.system("sox -v 0.80 '%s' -r 16000 -b 16 -c 1 %s" % (upsampled_wav_path, downsampled_wav_path)), which is the decrising of volume by 20% is almost removed these warings
 30) we may find a way to visualize attention mapping in our specific case of transformer used for ASR task.I guess, we will observe locality of this mapping.
 31) stage 2: for the case of 15000 utterances produced the following output 
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ò with 1
     ** Replaced 3 instances of OOVs with 1
     /espnet/egs/spanish_common_voice/asr1/../../../utils/data2json.sh --feat dump/test/deltafalse/feats.scp --bpecode data/lang_char/train_set_unigram5000.model data/test data/lang_char/train_set_unigram5000_units.txt
     /espnet/egs/spanish_common_voice/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/test/deltafalse/feats.scp data/test/tmp-12Uij/input_1/shape.scp
     skipped 0 empty lines
     filtered 0 lines
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing ő with 1
     sym2int.pl: replacing ồ with 1
     sym2int.pl: replacing ı with 1
     can it be fixed by proper preprocessing?
 32) we may optimize the data preparation phase as follows: reformat entire dataset according to kaldi, based on ESPNET_SUBSET_SIZE paramter select a subset the dataset for a specific experiment
 33) so, the common_voice description files are dev.tsv, invalidated.tsv, other.tsv, test.tsv, train.tsv, validated.tsv: what is invalidated.tsv? what is other.tsv? how the audio files where divided
     between the rest of the files? what is useful/importent for our case? 
     the fields in the train.tsv are: client_id, path, sentence, up_votes, down_votes, age, gender, accent. What are up_votesm, and down_votes? Can/should we do anything with age, gender, accent?
 34) Note: the download and reformating of the spanish_common_voice from mp3 22KHz to wav 16KHz takes ~7 hours
 35) stage 4 - acoustic model learning. 
     35.1) an attemt to run 16 GPUs did not work, but 8 GPU configuration worked flawlessly. I guess, the SW is not ready for 16 GPUs.
     35.2) 10 epochs: start_time: Sat Jun 27 12:13:50 UTC 2020, end_time: Sat Jun 27 17:29:44 UTC 2020 (on 8 GPUs). The recognition (the test) is bad
     35.3) 20 epochs: Ended (code 0) at Sun Jun 28 04:50:05 UTC 2020, elapsed time 38222 seconds. No signs of overfitting. The validation 'wer' plot looks like a knee: it has constant, 
           close to 100% value for 3.5K steps (6 hours), and then starts olmost linear descend up to 95% after 5.3k steps (10 hours). I would say that the 95% is to high in comparison to the 
           recognition phase results. Also the validation acc changes from 0.1 to almost 0.8, validation cer changes from 0.8 to 0.2, val_cer_ctc from 1 to 0.35, 
           val_loss_att drops from 100 to 20, val_loss_ctc drops from 120 to 40, val_loss drops from 100 to 30. Train acc and loss change monotonically each one in the right direction. 
           Once more: No signs of overfitting => more epochs should be productive

           The recognition is very meaninful:
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 62.7   25.4    11.9    6.8    44.1   95.8  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 64.0    31.6     4.5     7.3    43.3    95.1  |
  
     35.4) 40 epochs: 
           plots show signes of saturation, but I would do 10-20 more epochs to find point where the system starts overfitting 

           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 75.8   15.4     8.8    3.3    27.5   86.4  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 75.9    20.8     3.4     3.3    27.4    84.8  |
     35.5) 40 epochs
           ctc-weight: 0.4, lm-weight: 0.6, beam-size: 6 
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 81.3   13.0     5.8    4.8    23.5   69.4  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 82.6    15.1     2.4     2.3    19.8    66.9  |
     35.6) recognition with beam-seze 6 on 32 running CPU-cores (total numb of the cores is 64, but half of them are not active) takes 5088 sec
           recognition with beam-size 20 on 32 running CPU-cores (...)                                                          takes 15177 sec
     35.7) 40 epochs
           ctc-weight: 0.4, lm-weight: 0.6, beam-size: 20 - the results below should be verified
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt  # Wrd | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213   178861| 80.0   11.9     8.0    3.2    23.2   69.3  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 82.2    14.5     3.3     1.8    19.6    67.3  |

     35.8) train for 20 epochs of spanish_common_voice takes 38646 seconds == 10.735 hours
     35.9) 60 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 78.3   14.1     7.6    3.4    25.0   83.1  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 78.0    19.0     3.0     3.0    25.0    81.3  |

     35.10)80 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 79.1   13.3     7.6    2.9    23.8   80.7  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 79.0    18.0     3.0     2.6    23.7    79.0  |

     35.11)100 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 79.6   13.0     7.4    3.0    23.4   80.1  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 79.4    17.8     2.8     2.7    23.2    78.4  |

 36) model snapshots are in exp/train_set_pytorch_train/results/
 37) get model summary and visualization using pytorch and tensorboard apis
 38) selesction from the stage 5 log:
	dictionary: data/lang_char/train_set_unigram5000_units.txt


	stage 5: Decoding
	best val scores = [0.85404511 0.85185841 0.85165279 0.85099028 0.84982956]
	selected epochs = [98 97 92 91 89]
	average over ['exp/train_set_pytorch_train/results/snapshot.ep.98', 'exp/train_set_pytorch_train/results/snapshot.ep.97', 'exp/train_set_pytorch_train/results/snapshot.ep.92', 'exp/train_set_pytorch_train/results/snapshot.ep.91', 'exp/train_set_pytorch_train/results/snapshot.ep.89']
	2020-07-03 05:44:40,596 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/spanish_common_voice/asr1/../../../utils/splitjson.py --parts 32 dump/test/deltafalse/data_unigram5000.json
	2020-07-03 05:44:40,713 (splitjson:52) INFO: number of utterances = 11213
  39) how to copy the model:
      language model is here: exp/train_rnnlm_pytorch_lm_unigram5000_ngpu16/  
      acoustic model is here: exp/train_set_pytorch_train/results/
      lang_char model is here data/lang_char

  40) test set from Gong
          the model is taken from spanish_common_voice 100 epochs 
          ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

          write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
          | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
          | Sum/Avg                   |  187   2513 | 36.8   48.0   15.2   17.5   80.7   99.5 |
          write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
          | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
          | Sum/Avg                   |  187   1824 | 33.7   49.5   16.9    8.0   74.3   99.5 |


  41) train and test set is from Gong unsupervised
         the model is train on Gong unsupervised for 80 epochs
         ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                       | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                    | 8481  200714| 53.0   35.5   11.4    7.2   54.2   87.9 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                       | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                    | 8481  178196| 56.1   35.5    8.4    7.6   51.5   87.9 |

 
