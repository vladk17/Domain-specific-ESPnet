run.sh:
  1) set verbose=1 #line 18
  2) resume=        # Resume the training from snapshot #line 19
  3) TBD: verify we do not do spaceaugmentation at the first phase (preprocess_config=conf/specaug.yaml # line 25)
  4) study train.yaml params:  
     4.1) batch-bins should correspond to the number of gpus (default in librispeech is 15000000, current setting is 1000000)
     4.2) accum-grad: 4-default, current setting is 1
     4.3) epochs: 120-defualt, current 20
     4.4) model-module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E - TBD: study the code and model structure from the code
     4.5) meaning of many other params is not clear - TBD: clarify the meaning

  5) study lm.yaml params - TBD
  6) study decode.yaml params:
     6.1) beamsize 60-default, currently value is 5    
     6.2) ctc-weight vs. lm-weight, should they sumup to 1 or what's important is the ratio between them, or something else? TBD - check
  7) feature generation: # Generate the fbank features; by default 80-dimensional fbanks with pitch on each frame
     any ideas about speciality of spanish audio and corresponding tuning of the parameters of the feature generation for spanish?
  8) accelerate decoding by commenting out the line below #### use CPU for decoding
             8.1) #ngpu=0
             8.2) Set 1 or more values for --batchsize option in asr_recog.py to enable GPU decoding
             8.3) And execute the script (e.g., run.sh --stage 5 --ngpu 1)
             You’ll achieve significant speed improvement by using the GPU decoding
  9) TBD: clarify the meaning of "model to be used for decoding: 'model.acc.best' or 'model.loss.best'"
 10) TBD: prepare a schematic diagram of the pipeline and present it to the member of the project
 11) TBD: clarify what is CMVN (Cepstral mean and variance normalization), how is it calculated and how is it used? 
          remember: Mel-Frequency Cepstral Coefficients (MFCCs)
 12) TBD: exercise transfer learning on publically available data; do not wait for the gong's labeled data
 13) TBD: listen to the data audio files
 14) TBD: prepare a schematic diagram of the data transformations along the pipeline; debug the pipeline by verifications of the format and content correctness 
     of the data at each stage of the pipeline
 15) overfitting regularization loop
 16) stay in control at each stage of teh pipeline: 
     16.1) clearly understand the optimization cretarium at the stage; 
     16.2) measure and know the achieved optimization values
     16.3) do one stage a time, analyze the results; zero step is listen to the samples 
 17) in common voice, waveforms have significant portion os silence at the start and the end of the recording
 18) nbpe=5000 (this paramter defines the size of the vacabular for the stage 2 "Dictionary and Json Data Preparation") what is the recommended value of this param?
 19) bpemode=unigram define the model type in the stage 2 "Dictionary and Json Data Preparation"; what does it mean, what are the options, and what is the recommended model type?
     it is explained and implemented here https://arxiv.org/abs/1804.10959 and here https://github.com/google/sentencepiece and 
 20) stage 2: what is the meaning of the param --input_sentence_size=100000000
 21) stage 3: LM: lm.yaml model-module: transformer; epoch set to 20 while the default is 50; what are other options for model-module and preferable configuration settings?
 22) stage 3: get the exact meaning of the paramters in the log output: main/loss, main/count, main/nll
 23) stage 3: starting from 6th epoch the model starts to overfit in the case of small (5000 taken for development) dataset; echieved train perplexity is 57.3077, echieved 
     validation perplexity is 193.023 
 24) paper: A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATION (https://arxiv.org/abs/1909.06317) provides training tips; it also reveals superiority of transformer
     in comparison to RNN; they prepared espnet recipes, which we actually use (espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py by Shigeki Karita)
     in these recipes the model is sequence2sequence model, which predicts a target sequence of subwords ("SentencePiece") from an input sequence of (83-dim) log-mel filterbank frames with pitch features.
     Questions: how sensitive the model to the selected subwords-dict? What are the pros and cons of setting the alphabet characters as a subwords-dict (the minimal (?) "full",so that any word can
     be created, subwords-dict)?
     
     see "A pitch extraction algorithm tuned for automatic speech recognition" by Povey et all

     see "SPEECH-TRANSFORMER: A NO-RECURRENCE SEQUENCE-TO-SEQUENCE MODEL FOR SPEECH RECOGNITION", which explains transformations of the source sequence into a subsampled sequence by using 
     two layer cnn 
     
 25) not sure that the sub-words are better than characters for the asr: the length of the subwords varies significantly (they can be pretty long), which seem to be problematic 
     for the acustic model as it outputs per time-frame. We may try "characters" and mixed "character" + "subwords" and compare performance
 26) train.yaml:  
     26.1) mtlalpha: 0.3 (Multitask learning mode alpha) defines the weights between loss_att and loss_ctc in the joint loss function L_asr = alpha * loss_ctc + (1 - alpha) * loss_att, 
           where loss_xyz = -log(P_xyz); TBD: clarify how P_xyz is defined/calculated 
           see "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING"
           Note: the values of loss_att ~ 14, while ctc_loss ~ 28 after 40 epochs, so the value of mtlalpha seem to be selected correctly or can be even smaller
     26.2) num_spkrs: 1 - in our current setting (this assumes that there is only a single speaker in each utterance)
     26.3) model_module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E
     26.4) elayers: 12  defines the number of blocks in the transformer's encoder
     26.5) dlayers: 6 defines tne number of blocks in the transformer's decoder
           it looks like the CTC log below outputs the input and output sequence lengths for each of the (6) dlayers 
           "
           2020-06-26 06:48:15,536 (ctc:83) INFO: CTC input lengths:  tensor([154, 154, 154, 154, 154, 154], dtype=torch.int32)
           2020-06-26 06:48:15,536 (ctc:84) INFO: CTC output lengths: tensor([17, 17, 16,  9, 15, 21], dtype=torch.int32)
           "
     26.6) adim: 512 defines attention_dim - attantion dimention
     26.7) aheads: 8 - number of attention heads
     26.8) TBD: check what is the value of positionwise_layer_type and corresponding positionwise_layer, which is given as a feed_forward paramter to the Transformer's EncoderLayer
           from the code it looks that in the current implementation the default value is taken: positionwise_layer_type="linear",
 27) define and implement a data-cleaning mechanism, so that a new experiment could be started from the "clean page" 
 28) define and implement data-saving mechanism, so that results of an experiment could be saved 
 29) getting lot's of following wordings: "sox WARN rate: rate clipped 12 samples; decrease volume?", changing the command to os.system("sox -v 0.80 '%s' -r 16000 -b 16 -c 1 %s" % (upsampled_wav_path, downsampled_wav_path)), which is the decrising of volume by 20% is almost removed these warings
 30) we may find a way to visualize attention mapping in our specific case of transformer used for ASR task.I guess, we will observe locality of this mapping.
 31) stage 2: for the case of 15000 utterances produced the following output 
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ò with 1
     ** Replaced 3 instances of OOVs with 1
     /espnet/egs/spanish_common_voice/asr1/../../../utils/data2json.sh --feat dump/test/deltafalse/feats.scp --bpecode data/lang_char/train_set_unigram5000.model data/test data/lang_char/train_set_unigram5000_units.txt
     /espnet/egs/spanish_common_voice/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/test/deltafalse/feats.scp data/test/tmp-12Uij/input_1/shape.scp
     skipped 0 empty lines
     filtered 0 lines
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing ő with 1
     sym2int.pl: replacing ồ with 1
     sym2int.pl: replacing ı with 1
     can it be fixed by proper preprocessing?
 32) we may optimize the data preparation phase as follows: reformat entire dataset according to kaldi, based on ESPNET_SUBSET_SIZE paramter select a subset the dataset for a specific experiment
 33) so, the common_voice description files are dev.tsv, invalidated.tsv, other.tsv, test.tsv, train.tsv, validated.tsv: what is invalidated.tsv? what is other.tsv? how the audio files where divided
     between the rest of the files? what is useful/importent for our case? 
     the fields in the train.tsv are: client_id, path, sentence, up_votes, down_votes, age, gender, accent. What are up_votesm, and down_votes? Can/should we do anything with age, gender, accent?
 34) Note: the download and reformating of the spanish_common_voice from mp3 22KHz to wav 16KHz takes ~7 hours
 35) stage 4 - acoustic model learning. 
     35.1) an attemt to run 16 GPUs did not work, but 8 GPU configuration worked flawlessly. I guess, the SW is not ready for 16 GPUs.
     35.2) 10 epochs: start_time: Sat Jun 27 12:13:50 UTC 2020, end_time: Sat Jun 27 17:29:44 UTC 2020 (on 8 GPUs). The recognition (the test) is bad
     35.3) 20 epochs: Ended (code 0) at Sun Jun 28 04:50:05 UTC 2020, elapsed time 38222 seconds. No signs of overfitting. The validation 'wer' plot looks like a knee: it has constant, 
           close to 100% value for 3.5K steps (6 hours), and then starts olmost linear descend up to 95% after 5.3k steps (10 hours). I would say that the 95% is to high in comparison to the 
           recognition phase results. Also the validation acc changes from 0.1 to almost 0.8, validation cer changes from 0.8 to 0.2, val_cer_ctc from 1 to 0.35, 
           val_loss_att drops from 100 to 20, val_loss_ctc drops from 120 to 40, val_loss drops from 100 to 30. Train acc and loss change monotonically each one in the right direction. 
           Once more: No signs of overfitting => more epochs should be productive

           The recognition is meaninful:
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 62.7   25.4    11.9    6.8    44.1   95.8  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 64.0    31.6     4.5     7.3    43.3    95.1  |
  
     35.4) 40 epochs: 
           plots show signes of saturation, but I would do 10-20 more epochs to find point where the system starts overfitting 

           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 75.8   15.4     8.8    3.3    27.5   86.4  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 75.9    20.8     3.4     3.3    27.4    84.8  |
     35.5) 40 epochs
           ctc-weight: 0.4, lm-weight: 0.6, beam-size: 6 
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 81.3   13.0     5.8    4.8    23.5   69.4  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 82.6    15.1     2.4     2.3    19.8    66.9  |
     35.6) recognition with beam-seze 6 on 32 running CPU-cores (total numb of the cores is 64, but half of them are not active) takes 5088 sec
           recognition with beam-size 20 on 32 running CPU-cores (...)                                                          takes 15177 sec
     35.7) 40 epochs
           ctc-weight: 0.4, lm-weight: 0.6, beam-size: 20 - the results below should be verified
           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt  # Wrd | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213   178861| 80.0   11.9     8.0    3.2    23.2   69.3  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 82.2    14.5     3.3     1.8    19.6    67.3  |

     35.8) train for 20 epochs of spanish_common_voice takes 38646 seconds == 10.735 hours
     35.9) 60 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 78.3   14.1     7.6    3.4    25.0   83.1  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 78.0    19.0     3.0     3.0    25.0    81.3  |

     35.10)80 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 79.1   13.3     7.6    2.9    23.8   80.7  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 79.0    18.0     3.0     2.6    23.7    79.0  |

     35.11)100 epochs
           ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

           write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
           | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
           | Sum/Avg         |11213  178861 | 79.6   13.0     7.4    3.0    23.4   80.1  |
           write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
           | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
           | Sum/Avg         |11213   107268 | 79.4    17.8     2.8     2.7    23.2    78.4  |

 36) model snapshots are in exp/train_set_pytorch_train/results/
 37) get model summary and visualization using pytorch and tensorboard apis
 38) selesction from the stage 5 log:
	dictionary: data/lang_char/train_set_unigram5000_units.txt


	stage 5: Decoding
	best val scores = [0.85404511 0.85185841 0.85165279 0.85099028 0.84982956]
	selected epochs = [98 97 92 91 89]
	average over ['exp/train_set_pytorch_train/results/snapshot.ep.98', 'exp/train_set_pytorch_train/results/snapshot.ep.97', 'exp/train_set_pytorch_train/results/snapshot.ep.92', 'exp/train_set_pytorch_train/results/snapshot.ep.91', 'exp/train_set_pytorch_train/results/snapshot.ep.89']
	2020-07-03 05:44:40,596 (splitjson:40) INFO: /espnet/tools/venv/bin/python3 /espnet/egs/spanish_common_voice/asr1/../../../utils/splitjson.py --parts 32 dump/test/deltafalse/data_unigram5000.json
	2020-07-03 05:44:40,713 (splitjson:52) INFO: number of utterances = 11213
  39) how to copy the model:
      language model is here: exp/train_rnnlm_pytorch_lm_unigram5000_ngpu16/  
      acoustic model is here: exp/train_set_pytorch_train/results/
      lang_char model is here data/lang_char

  40) test set from Gong "second path"
          the model is taken from spanish_common_voice 100 epochs 
          ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

          write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
          | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
          | Sum/Avg                   |  187   2513 | 36.8   48.0   15.2   17.5   80.7   99.5 |
          write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
          | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
          | Sum/Avg                   |  187   1824 | 33.7   49.5   16.9    8.0   74.3   99.5 |


  41) train and test set is from Gong unsupervised
         the model is trained on Gong unsupervised for 80 epochs
         ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                       | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                    | 8481  200714| 53.0   35.5   11.4    7.2   54.2   87.9 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                       | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                    | 8481  178196| 56.1   35.5    8.4    7.6   51.5   87.9 |

  42) test set is from Gong "second path"
         the model is trained on Gong unsupervised for 80 epochs
         ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   2513 | 39.6   29.4   31.0    1.6   62.0   98.4 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   1824 | 58.6   29.2   12.3    4.6   46.1   96.3 |

  43) planned experiments: 
      43.1) train lm and am for spanish_common_voice, while the normalized subwords-dict is taken from gong_unsupervised; test on gong_test  
      43.2) train am for spanish_common_voice, while subwirds-dict is taken from gong unsupervised; test on gong_test using this am in combination with lm from gong-unsupervised
      43.3) reduce the size of the subwords dict to the size of alphabet (or a bit higher), verify that it has all the charaters of the alphabet. 
            learn the cspanish_common_voice and spanish_cong_unsupervised with this subword-dict and see the results 
      43.4) take the best spanish_common_voice model (with subwords-dict from spanish_gong_unsupervised) and do tranfer learn for gong first_path or subset of the gong second_path.
            try the following transfer-learning strategies: 
            a) unfreeze the last layer(s) of the decoder attention block; 
            b) unfreeze the last layer(s) of the encoder attention block; 
            c) unfreeze the last layer(s) of the encoder-decoder attention block;
            d) unfreeze everything but use very small learning rate
   44) implementing 43.1) "train lm and am for spanish_common_voice, while the subwords-dict is taken from gong_unsupervised; test on gong_test"
      44.1) lm training:
            epoch       iteration   main/loss   perplexity  val_perplexity  elapsed_time
            0           100         6.60049     728.885                     92.9827       
            0           200         5.8641      338.312                     144.886       
            0           300         5.42299     215.843                     197.603       
            1           400         5.16045     169.309     161.09          262.026       
            2           800         4.50149     88.5554     96.7707         487.134       
            3           1100        4.1431      62.433      71.9046         659.578       
            4           1500        3.75335     44.0624     59.7008         884.279       
            5           1800        3.49552     34.461      52.7506         1056.31       
            6           2200        3.15814     25.8888     48.8274         1280.05       
            7           2500        2.96206     21.2424     47.3234         1453.43       
            8           2900        2.69402     16.7335     47.6169         1678.61       
            9           3300        2.45741     13.2071     48.9845         1905.24       

      44.2) test set is from spanish_common_voice
            the model is "train lm and am for spanish_common_voice, while the subwords-dict is taken from gong_unsupervised after 60 epochs
            ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
         | Sum/Avg         |11213  172519 | 83.8   11.5     4.7    2.3    18.5   70.5  |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
         | Sum/Avg         |11213   106544 | 82.7    14.8     2.5     2.2    19.5    70.0  |


      44.3) test set is from spanish_common_voice 
            the model is "train lm and am for spanish_common_voice, while the subwords-dict is taken from gong_unsupervised after 80 epochs
            ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR            | # Snt # Wrd  | Corr    Sub     Del    Ins     Err  S.Err  |
         | Sum/Avg         |11213  172519 | 84.7   10.8     4.5    2.0    17.3   67.4  |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR            | # Snt  # Wrd  | Corr     Sub     Del     Ins     Err   S.Err  |
         | Sum/Avg         |11213   106544 | 83.8    13.9     2.3     2.2    18.4    66.9  |

      44.4) test set is from gong second pass 
            the model is "train lm and am for spanish_common_voice, while the subwords-dict is taken from gong_unsupervised after 80 epochs
            ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   2513 | 30.2   42.1   27.7    5.3   75.0   99.5 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   1824 | 38.5   40.1   21.3    4.3   65.8   97.3 |

      44.5) test set is from gong second pass
            the model is "train lm and am for spanish_common_voice, while tha subwods-dict is taken from gong_unsupervised after 80 epochs
            ctc-weigth: 0.4, lm-weight: 0.6, beam-size: 6

         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   2513 | 30.8   44.1   25.1    7.7   76.8   99.5 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   1824 | 39.5   40.0   20.6    5.2   65.7   98.4 |

      44.6) test set is from gong second pass
            the model is "train lm" on gong unsupervised "train am"-80 epochs  on spanish_common_voice, while subwords-dict is taken from gong_unsupervosed 
            ctc-weight: 0.4, lm-weight: 0.6, beam-size: 6
         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   2513 | 32.9   39.6   27.5    7.0   74.1   99.5 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   1824 | 43.6   37.0   19.4    6.1   62.5   97.9 |
         the result above is the best for the "cross-learning", where hight quality am for spanish_common_voice is directly used for gong_second_pass predictions
         looks like the only way to effectively use the public data-sets is in some tranfer-learning setup

      44.7) test set is from gong second pass
            the model is "train lm" on gong unsupervised "train am"-80 epochs  on spanish_common_voice, while subwords-dict is taken from gong_unsupervosed
            ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
         write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   2513 | 30.4   42.1   27.5    5.3   74.9   99.5 |
         write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
         | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
         | Sum/Avg                   |  187   1824 | 38.8   40.1   21.2    4.4   65.6   97.3 |



   45) from Stas: test set is Gong "second path"
       the model is trained on Gong unsupervised + part of the "second path" after 60 epochs 

       ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
        | SPKR                      | # Snt # Wrd  | Corr    Sub    Del    Ins     Err  S.Err |
        | Sum/Avg                   |  187   1824  | 67.1   24.7    8.3    8.4    41.4   93.6 |

       ctc-weight: 0.4, lm-weight: 0.6, beam-size: 20
        | SPKR                      | # Snt # Wrd  | Corr    Sub    Del    Ins     Err  S.Err |
        | Sum/Avg                   |  187   1824  | 58.3   24.1   17.5    3.5    45.1   96.8 |

        lm training: 
        epoch       iteration   main/loss   perplexity  val_perplexity  elapsed_time
        1           400         5.53858     260.304     238.009         158.518
        2           800         5.03653     157.693     160.119         308.356
        3           1200        4.70864     113.94      129.4           459.669
        4           1600        4.41538     85.9739     114.145         608.809
        5           2000        4.14418     66.4352     106.376         758.732
        6           2400        3.86634     50.146      103.094         907.932
        7           2800        3.56963     37.3255     101.225         1057.31
        8           3100        3.37352     31.382      105.839         1171.54
        9           3500        3.09786     24.0359     111.113         1321.82

   46) "tedx" with specaug (train set is 21.955 hours, test set is 2.5 hours)
      46.1) lm training: - note val perplexity is worse than in the "gong" and "common voice"
        epoch       iteration   main/loss   perplexity  val_perplexity  elapsed_time
        2           100         6.42578     613.325     477.262         111.274       
        4           200         5.43248     227.754     266.845         178.378       
        7           300         4.95533     142.897     200.695         247.84        
        9           400         4.62064     103.813     180.136         315.331       
        12          500         4.29036     75.2354     174.926         384.567       
        14          600         3.95026     54.9311     187.093         451.41        
        17          700         3.61729     39.5093     211.163         520.277       
        19          800         3.27504     28.6317     248.92          585.976        

       the model is trained on "tedx" and tested on "tedx"
      46.2)
       the model is trained on "tedx" with specaug for 160 epochs and tested on "tedx"
       ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
       write a CER (or TER) result in exp/train_set_pytorch_train_specaug/decode_test_model.val5.avg.best_decode_lm/result.txt
       |  SPKR        |  # Snt   # Wrd  |  Corr      Sub      Del     Ins      Err    S.Err  |
       |  Sum/Avg     |  1125    30383  |  54.0     31.5     14.5     6.7     52.7     99.7  |
       write a WER result in exp/train_set_pytorch_train_specaug/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
       |  SPKR        |  # Snt    # Wrd   |  Corr      Sub      Del      Ins       Err    S.Err  |
       |  Sum/Avg     |  1125     23403   |  58.2     33.2      8.6      7.5      49.3     99.6  |

      46.3) 
      the model is trained on "tedx" with specaug for 160 epochs and tested on "gong second pass"
      ctc-weight: 0.99, lm-weight: 0.01, beam-size: 6
      write a CER (or TER) result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.txt
      | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
      | Sum/Avg                   |  187   2513 | 27.1   57.3   15.6   35.6  108.6  100.0 |
      write a WER result in exp/train_set_pytorch_train/decode_test_model.val5.avg.best_decode_lm/result.wrd.txt
      | SPKR                      | # Snt # Wrd | Corr    Sub    Del    Ins    Err  S.Err |
      | Sum/Avg                   |  187   1824 | 34.5   56.9    8.6   46.8  112.2  100.0 |


   47) common voice duration: train 147.6 hours, test 17.5 hours
   48) gong unsupervised duration: train 175 hours, test 18.76 hours

   49) "crowdspurce" (train set is 22.64 hours, test set is 2.3 hours) 
      49.1) lm training 
      epoch       iteration   main/loss   perplexity  val_perplexity  elapsed_time
      1           100         6.01156     450.579     378.094         99.2414       
      3           200         3.97982     65.3992     89.5308         160.853       
      5           300         2.87604     22.2586     30.1342         222.536       
      7           400         2.11761     10.2103     13.413          284.582       
      9           500         1.62016     5.9073      7.65594         346.138       
      11          600         1.30324     4.09022     5.15334         409.097       
      13          700         1.10439     3.19253     3.90788         470.463       
      15          800         0.983358    2.71573     3.27007         532.113       
      17          900         0.912814    2.48546     2.90892         594.318       
      19          1000        0.859612    2.31353     2.65487         656.377       
      21          1100        0.822165    2.19253     2.4923          717.422       
      23          1200        0.786432    2.09148     2.34582         777.923       
      25          1300        0.768305    2.0328      2.27833         838.22        
      27          1400        0.75121     1.98469     2.23873         898.917       
      29          1500        0.73268     1.94088     2.17974         959.597   

[========== our am:
E2E(
  (encoder): Encoder(
    (embed): Conv2dSubsampling(
      (conv): Sequential(
        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))
        (1): ReLU()
        (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))
        (3): ReLU()
      )
      (out): Sequential(
        (0): Linear(in_features=10240, out_features=512, bias=True)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1)
        )
      )
    )
    (encoders): MultiSequential(
      (0): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (1): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (2): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (3): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (4): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (5): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (6): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (7): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (8): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (9): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (10): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (11): EncoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
    )
    (after_norm): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
  )
  (decoder): Decoder(
    (embed): Sequential(
      (0): Embedding(5002, 512)
      (1): PositionalEncoding(
        (dropout): Dropout(p=0.1)
      )
    )
    (decoders): MultiSequential(
      (0): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (1): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (2): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (3): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (4): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
      (5): DecoderLayer(
        (self_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (src_attn): MultiHeadedAttention(
          (linear_q): Linear(in_features=512, out_features=512, bias=True)
          (linear_k): Linear(in_features=512, out_features=512, bias=True)
          (linear_v): Linear(in_features=512, out_features=512, bias=True)
          (linear_out): Linear(in_features=512, out_features=512, bias=True)
          (dropout): Dropout(p=0.0)
        )
        (feed_forward): PositionwiseFeedForward(
          (w_1): Linear(in_features=512, out_features=2048, bias=True)
          (w_2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout): Dropout(p=0.1)
        )
        (norm1): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm2): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (norm3): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1)
      )
    )
    (after_norm): LayerNorm(torch.Size([512]), eps=1e-12, elementwise_affine=True)
    (output_layer): Linear(in_features=512, out_features=5002, bias=True)
  )
  (criterion): LabelSmoothingLoss(
    (criterion): KLDivLoss()
  )
  (ctc): CTC(
    (ctc_lo): Linear(in_features=512, out_features=5002, bias=True)
    (ctc_loss): CTCLoss()
  )
)
]==========

[=========am layers' sizes
   415 2020-07-11 17:55:27,133 (recog:50) INFO: torch.Size([512, 1, 3, 3])
    416 2020-07-11 17:55:27,133 (recog:50) INFO: torch.Size([512])
    417 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 512, 3, 3])
    418 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    419 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 10240])
    420 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    421 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 512])
    422 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    423 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 512])
    424 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    425 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 512])
    426 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    427 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512, 512])
    428 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([512])
    429 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([2048, 512])
    430 2020-07-11 17:55:27,134 (recog:50) INFO: torch.Size([2048])
    431 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512, 2048])
    432 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    433 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    434 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    435 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    436 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    437 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512, 512])
    438 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    439 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512, 512])
    440 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    441 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512, 512])
    442 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    443 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512, 512])
    444 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([512])
    445 2020-07-11 17:55:27,135 (recog:50) INFO: torch.Size([2048, 512])
    446 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([2048])
    447 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512, 2048])
    448 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    449 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    450 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    451 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    452 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    453 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512, 512])
    454 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    455 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512, 512])
    456 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    457 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512, 512])
    458 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    459 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512, 512])
    460 2020-07-11 17:55:27,136 (recog:50) INFO: torch.Size([512])
    461 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([2048, 512])
    462 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([2048])
    463 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512, 2048])
    464 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    465 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    466 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    467 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    468 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    469 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512, 512])
    470 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    471 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512, 512])
    472 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    473 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512, 512])
    474 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512])
    475 2020-07-11 17:55:27,137 (recog:50) INFO: torch.Size([512, 512])
    476 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    477 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([2048, 512])
    478 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([2048])
    479 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512, 2048])
    480 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    481 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    482 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    483 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    484 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    485 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512, 512])
    486 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    487 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512, 512])
    488 2020-07-11 17:55:27,138 (recog:50) INFO: torch.Size([512])
    489 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512, 512])
    490 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    491 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512, 512])
    492 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    493 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([2048, 512])
    494 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([2048])
    495 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512, 2048])
    496 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    497 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    498 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    499 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    500 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    501 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512, 512])
    502 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512])
    503 2020-07-11 17:55:27,139 (recog:50) INFO: torch.Size([512, 512])
    504 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    505 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512, 512])
    506 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    507 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512, 512])
    508 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    509 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([2048, 512])
    510 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([2048])
    511 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512, 2048])
    512 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    513 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    514 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    515 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    516 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    517 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512, 512])
    518 2020-07-11 17:55:27,140 (recog:50) INFO: torch.Size([512])
    519 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512, 512])
    520 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    521 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512, 512])
    522 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    523 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512, 512])
    524 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    525 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([2048, 512])
    526 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([2048])
    527 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512, 2048])
    528 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    529 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    530 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    531 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    532 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512])
    533 2020-07-11 17:55:27,141 (recog:50) INFO: torch.Size([512, 512])
    534 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    535 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512, 512])
    536 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    537 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512, 512])
    538 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    539 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512, 512])
    540 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    541 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([2048, 512])
    542 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([2048])
    543 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512, 2048])
    544 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    545 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    546 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    547 2020-07-11 17:55:27,142 (recog:50) INFO: torch.Size([512])
    548 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    549 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512, 512])
    550 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    551 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512, 512])
    552 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    553 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512, 512])
    554 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    555 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512, 512])
    556 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    557 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([2048, 512])
    558 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([2048])
    559 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512, 2048])
    560 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    561 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    562 2020-07-11 17:55:27,143 (recog:50) INFO: torch.Size([512])
    563 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    564 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    565 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512, 512])
    566 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    567 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512, 512])
    568 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    569 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512, 512])
    570 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    571 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512, 512])
    572 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    573 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([2048, 512])
    574 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([2048])
    575 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512, 2048])
    576 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    577 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    578 2020-07-11 17:55:27,144 (recog:50) INFO: torch.Size([512])
    579 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    580 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    581 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512, 512])
    582 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    583 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512, 512])
    584 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    585 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512, 512])
    586 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    587 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512, 512])
    588 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    589 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([2048, 512])
    590 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([2048])
    591 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512, 2048])
    592 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    593 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    594 2020-07-11 17:55:27,145 (recog:50) INFO: torch.Size([512])
    595 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    596 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    597 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512, 512])
    598 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    599 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512, 512])
    600 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    601 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512, 512])
    602 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    603 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512, 512])
    604 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    605 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([2048, 512])
    606 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([2048])
    607 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512, 2048])
    608 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    609 2020-07-11 17:55:27,146 (recog:50) INFO: torch.Size([512])
    610 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    611 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    612 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    613 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    614 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    615 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([5002, 512])
    616 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512, 512])
    617 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    618 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512, 512])
    619 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    620 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512, 512])
    621 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    622 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512, 512])
    623 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512])
    624 2020-07-11 17:55:27,147 (recog:50) INFO: torch.Size([512, 512])
    625 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    626 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512, 512])
    627 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    628 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512, 512])
    629 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    630 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512, 512])
    631 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    632 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([2048, 512])
    633 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([2048])
    634 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512, 2048])
    635 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    636 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    637 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    638 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    639 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    640 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    641 2020-07-11 17:55:27,148 (recog:50) INFO: torch.Size([512])
    642 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    643 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    644 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    645 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    646 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    647 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    648 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    649 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    650 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    651 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    652 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    653 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    654 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    655 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    656 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512, 512])
    657 2020-07-11 17:55:27,149 (recog:50) INFO: torch.Size([512])
    658 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([2048, 512])
    659 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([2048])
    660 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512, 2048])
    661 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    662 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    662 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    663 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    664 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    665 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    666 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    667 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    668 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512, 512])
    669 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    670 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512, 512])
    671 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512])
    672 2020-07-11 17:55:27,150 (recog:50) INFO: torch.Size([512, 512])
    673 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    674 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 512])
    675 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    676 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 512])
    677 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    678 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 512])
    679 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    680 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 512])
    681 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    682 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 512])
    683 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    684 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([2048, 512])
    685 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([2048])
    686 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512, 2048])
    687 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    688 2020-07-11 17:55:27,151 (recog:50) INFO: torch.Size([512])
    689 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    690 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    691 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    692 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    693 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    694 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    695 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    696 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    697 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    698 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    699 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    700 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    701 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    702 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    703 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    704 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512, 512])
    705 2020-07-11 17:55:27,152 (recog:50) INFO: torch.Size([512])
    706 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512, 512])
    707 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    708 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512, 512])
    709 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    710 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([2048, 512])
    711 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([2048])
    712 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512, 2048])
    713 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    714 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    715 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    716 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    717 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    718 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    719 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    720 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512, 512])
    721 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512])
    722 2020-07-11 17:55:27,153 (recog:50) INFO: torch.Size([512, 512])
    723 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    724 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    725 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    726 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    727 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    728 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    729 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    730 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    731 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    732 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    733 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    734 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 512])
    735 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512])
    736 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([2048, 512])
    737 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([2048])
    738 2020-07-11 17:55:27,154 (recog:50) INFO: torch.Size([512, 2048])
    739 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    740 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    741 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    742 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    743 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    744 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    745 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    746 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512, 512])
    747 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    748 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512, 512])
    749 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    750 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512, 512])
    751 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    752 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512, 512])
    753 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512])
    754 2020-07-11 17:55:27,155 (recog:50) INFO: torch.Size([512, 512])
    755 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    756 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512, 512])
    757 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    758 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512, 512])
    759 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    760 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512, 512])
    761 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    762 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([2048, 512])
    763 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([2048])
    764 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512, 2048])
    765 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    766 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    767 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    768 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    769 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    770 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    771 2020-07-11 17:55:27,156 (recog:50) INFO: torch.Size([512])
    772 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([512])
    773 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([512])
    774 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([5002, 512])
    775 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([5002])
    776 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([5002, 512])
    777 2020-07-11 17:55:27,157 (recog:50) INFO: torch.Size([5002])
]=========am layers' sizes





