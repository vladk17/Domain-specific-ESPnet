run.sh:
  1) set verbose=1 #line 18
  2) resume=        # Resume the training from snapshot #line 19
  3) TBD: verify we do not do spaceaugmentation at the first phase (preprocess_config=conf/specaug.yaml # line 25)
  4) study train.yaml params:  
     4.1) batch-bins should correspond to the number of gpus (default in librispeech is 15000000, current setting is 1000000)
     4.2) accum-grad: 4-default, current setting is 1
     4.3) epochs: 120-defualt, current 20
     4.4) model-module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E - TBD: study the code and model structure from the code
     4.5) meaning of many other params is not clear - TBD: clarify the meaning

  5) study lm.yaml params - TBD
  6) study decode.yaml params:
     6.1) beamsize 60-default, currently value is 5    
     6.2) ctc-weight vs. lm-weight, should they sumup to 1 or what's important is the ratio between them, or something else? TBD - check
  7) feature generation: # Generate the fbank features; by default 80-dimensional fbanks with pitch on each frame
     any ideas about speciality of spanish audio and corresponding tuning of the parameters of the feature generation for spanish?
  8) accelerate decoding by commenting out the line below #### use CPU for decoding
             8.1) #ngpu=0
             8.2) Set 1 or more values for --batchsize option in asr_recog.py to enable GPU decoding
             8.3) And execute the script (e.g., run.sh --stage 5 --ngpu 1)
             You’ll achieve significant speed improvement by using the GPU decoding
  9) TBD: clarify the meaning of "model to be used for decoding: 'model.acc.best' or 'model.loss.best'"
 10) TBD: prepare a schematic diagram of the pipeline and present it to the member of the project
 11) TBD: clarify what is CMVN (Cepstral mean and variance normalization), how is it calculated and how is it used? 
          remember: Mel-Frequency Cepstral Coefficients (MFCCs)
 12) TBD: exercise transfer learning on publically available data; do not wait for the gong's labeled data
 13) TBD: listen to the data audio files
 14) TBD: prepare a schematic diagram of the data transformations along the pipeline; debug the pipeline by verifications of the format and content correctness 
     of the data at each stage of the pipeline
 15) overfitting regularization loop
 16) stay in control at each stage of teh pipeline: 
     16.1) clearly understand the optimization cretarium at the stage; 
     16.2) measure and know the achieved optimization values
     16.3) do one stage a time, analyze the results; zero step is listen to the samples 
 17) in common voice, waveforms have significant portion os silence at the start and the end of the recording
 18) nbpe=5000 (this paramter defines the size of the vacabular for the stage 2 "Dictionary and Json Data Preparation") what is the recommended value of this param?
 19) bpemode=unigram define the model type in the stage 2 "Dictionary and Json Data Preparation"; what does it mean, what are the options, and what is the recommended model type?
     it is explained and implemented here https://arxiv.org/abs/1804.10959 and here https://github.com/google/sentencepiece and 
 20) stage 2: what is the meaning of the param --input_sentence_size=100000000
 21) stage 3: LM: lm.yaml model-module: transformer; epoch set to 20 while the default is 50; what are other options for model-module and preferable configuration settings?
 22) stage 3: get the exact meaning of the paramters in the log output: main/loss, main/count, main/nll
 23) stage 3: starting from 6th epoch the model starts to overfit in the case of small (5000 taken for development) dataset; echieved train perplexity is 57.3077, echieved 
     validation perplexity is 193.023 
 24) paper: A COMPARATIVE STUDY ON TRANSFORMER VS RNN IN SPEECH APPLICATION (https://arxiv.org/abs/1909.06317) provides training tips; it also reveals superiority of transformer
     in comparison to RNN; they prepared espnet recipes, which we actually use (espnet/espnet/nets/pytorch_backend/e2e_asr_transformer.py by Shigeki Karita)
     in these recipes the model is sequence2sequence model, which predicts a target sequence of subwords ("SentencePiece") from an input sequence of (83-dim) log-mel filterbank frames with pitch features.
     
     see "A pitch extraction algorithm tuned for automatic speech recognition" by Povey et all

     see "SPEECH-TRANSFORMER: A NO-RECURRENCE SEQUENCE-TO-SEQUENCE MODEL FOR SPEECH RECOGNITION", which explains transformations of the source sequence into a subsampled sequence by using 
     two layer cnn 
     
 25) not sure that the sub-words are better than characters for the asr: the length of the subwords varies significantly (they can be pretty long), which seem to be problematic 
     for the acustic model as it outputs per time-frame. We may try "characters" and mixed "character" + "subwords" and compare performance
 26) train.yaml:  
     26.1) mtlalpha: 0.3 (Multitask learning mode alpha) defines the weights between loss_att and loss_ctc in the joint loss function L_asr = alpha * loss_ctc + (1 - alpha) * loss_att, 
           where loss_xyz = log(P_xyz)
           see "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING"
     26.2) num_spkrs: 1 - in our current setting (this assumes that there is only a single speaker in each utterance)
     26.3) model_module: espnet.nets.pytorch_backend.e2e_asr_transformer:E2E
     26.4) elayers: 12  defines the number of blocks in the transformer's encoder
     26.5) dlayers: 6 defines tne number of blocks in the transformer's decoder
           it looks like the CTC log below outputs the input and output sequence lengths for each of the (6) dlayers 
           "
           2020-06-26 06:48:15,536 (ctc:83) INFO: CTC input lengths:  tensor([154, 154, 154, 154, 154, 154], dtype=torch.int32)
           2020-06-26 06:48:15,536 (ctc:84) INFO: CTC output lengths: tensor([17, 17, 16,  9, 15, 21], dtype=torch.int32)
           "
     26.6) adim: 512 defines attention_dim - attantion dimention
     26.7) aheads: 8 - number of attention heads
     26.8) TBD: check what is the value of positionwise_layer_type and corresponding positionwise_layer, which is given as a feed_forward paramter to the Transformer's EncoderLayer
           from the code it looks that in the current implementation the default value is taken: positionwise_layer_type="linear",
 27) define and implement a data-cleaning mechanism, so that a new experiment could be started from the "clean page" 
 28) define and implement data-saving mechanism, so that results of an experiment could be saved 
 29) getting lot's of following wordings: "sox WARN rate: rate clipped 12 samples; decrease volume?", changing the command to os.system("sox -v 0.80 '%s' -r 16000 -b 16 -c 1 %s" % (upsampled_wav_path, downsampled_wav_path)), which is the decrising of volume by 20% is almost removed these warings
 30) we may find a way to visualize attention mapping in our specific case of transformer used for ASR task.I guess, we will observe locality of this mapping.
 31) stage 2: for the case of 15000 utterances produced the following output 
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ı with 1
     sym2int.pl: replacing ò with 1
     ** Replaced 3 instances of OOVs with 1
     /espnet/egs/spanish_common_voice/asr1/../../../utils/data2json.sh --feat dump/test/deltafalse/feats.scp --bpecode data/lang_char/train_set_unigram5000.model data/test data/lang_char/train_set_unigram5000_units.txt
     /espnet/egs/spanish_common_voice/asr1/../../../utils/feat_to_shape.sh --cmd run.pl --nj 1 --filetype  --preprocess-conf  --verbose 0 dump/test/deltafalse/feats.scp data/test/tmp-12Uij/input_1/shape.scp
     skipped 0 empty lines
     filtered 0 lines
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing — with 1
     sym2int.pl: replacing ő with 1
     sym2int.pl: replacing ồ with 1
     sym2int.pl: replacing ı with 1
     can it be fixed by proper preprocessing?
 32) we may optimize the data preparation phase as follows: reformat entire dataset according to kaldi, based on ESPNET_SUBSET_SIZE paramter select a subset the dataset for a specific experiment
 33) so, the common_voice description files are dev.tsv, invalidated.tsv, other.tsv, test.tsv, train.tsv, validated.tsv: what is invalidated.tsv? what is other.tsv? how the audio files where divided
     between the rest of the files? what is useful/importent for our case? 
     the fields in the train.tsv are: client_id, path, sentence, up_votes, down_votes, age, gender, accent. What are up_votesm, and down_votes? Can/should we do anything with age, gender, accent?
 34) Note: the download and reformating of the spanish_common_voice from mp3 22KHz to wav 16KHz takes ~7 hours
 35) stage 4 - acoustic model learning. 
     35.1) an attemt to run 16 GPUs did not work, but 8 GPU configuration worked flawlessly. I guess, the SW is not ready for 16 GPUs.
     35.2) 10 epochs: start_time: Sat Jun 27 12:13:50 UTC 2020, end_time: Sat Jun 27 17:29:44 UTC 2020 (on 8 GPUs) - the recognition is bad
     35.3) 20 epochs: Ended (code 0) at Sun Jun 28 04:50:05 UTC 2020, elapsed time 38222 seconds. No signs of overfitting 
 36)  
